## 7.1 å¤§æ¨¡å‹ä¹‹æ¨¡å‹æ¦‚æ‹¬

è¯­è¨€æ¨¡å‹çš„ä¸€å¼€å§‹å°±å¯ä»¥è¢«çœ‹åšæ˜¯ä¸€ä¸ªé»‘ç®±ï¼Œå½“å‰å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›åœ¨äºç»™å®šä¸€ä¸ªåŸºäºè‡ªèº«éœ€æ±‚çš„promptå°±å¯ä»¥ç”Ÿæˆç¬¦åˆéœ€æ±‚çš„ç»“æœã€‚å½¢å¼å¯ä»¥è¡¨è¾¾ä¸ºï¼š

$$
prompt \leadsto completion
$$

ä»æ•°å­¦çš„è§’åº¦è€ƒè™‘å°±å¯¹è®­ç»ƒæ•°æ® (traing data:  $ï¼ˆx_{1},â€¦,x_{L}ï¼‰$ )çš„æ¦‚ç‡åˆ†å¸ƒï¼š

$$
trainingData \Rightarrow p(x_{1},...,x_{L}).
$$

åœ¨å­¦ä¹ å†…å®¹ä¸­ä¸­ï¼Œæˆ‘ä»¬å°†å½»åº•æ­å¼€é¢çº±ï¼Œè®¨è®ºå¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦‚ä½•æ„å»ºçš„ã€‚ä»Šå¤©çš„å†…å®¹å°†ç€é‡è®¨è®ºä¸¤ä¸ªä¸»é¢˜ï¼Œåˆ†åˆ«æ˜¯åˆ†è¯å’Œæ¨¡å‹æ¶æ„ï¼š

- åˆ†è¯ï¼šå³å¦‚ä½•å°†ä¸€ä¸ªå­—ç¬¦ä¸²æ‹†åˆ†æˆå¤šä¸ªæ ‡è®°ã€‚

- æ¨¡å‹æ¶æ„ï¼šæˆ‘ä»¬å°†ä¸»è¦è®¨è®ºTransformeræ¶æ„ï¼Œè¿™æ˜¯çœŸæ­£å®ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„å»ºæ¨¡åˆ›æ–°ã€‚

## 7.2 åˆ†è¯

å›é¡¾åˆšæ‰çš„å†…å®¹ï¼Œè¯­è¨€æ¨¡å‹ $p$ æ˜¯ä¸€ä¸ªå¯¹æ ‡è®°ï¼ˆtokenï¼‰åºåˆ—çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå…¶ä¸­æ¯ä¸ªæ ‡è®°æ¥è‡ªæŸä¸ªè¯æ±‡è¡¨ $V$ï¼š

$$ [the, mouse, ate, the, cheese] $$

ç„¶è€Œï¼Œè‡ªç„¶è¯­è¨€å¹¶ä¸æ˜¯ä»¥æ ‡è®°åºåˆ—çš„å½¢å¼å‡ºç°ï¼Œè€Œæ˜¯ä»¥å­—ç¬¦ä¸²çš„å½¢å¼å­˜åœ¨ï¼ˆå…·ä½“æ¥è¯´ï¼Œæ˜¯Unicodeå­—ç¬¦çš„åºåˆ—ï¼‰ï¼Œæ¯”å¦‚ä¸Šé¢çš„åºåˆ—çš„è‡ªç„¶è¯­è¨€ä¸ºâ€œ**the mouse ate the cheese**â€ã€‚

åˆ†è¯å™¨å°†ä»»æ„å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ ‡è®°åºåˆ—ï¼šthe mouse ate the cheese $\Rightarrow [the, mouse, ate, the, cheese]$ 

è¿™å¹¶ä¸ä¸€å®šæ˜¯è¯­è¨€å»ºæ¨¡ä¸­æœ€å¼•äººæ³¨ç›®çš„éƒ¨åˆ†ï¼Œä½†åœ¨ç¡®å®šæ¨¡å‹çš„å·¥ä½œæ•ˆæœæ–¹é¢èµ·ç€éå¸¸é‡è¦çš„ä½œç”¨ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥å°†è¿™ä¸ªæ–¹å¼ç†è§£ä¸ºè‡ªç„¶è¯­è¨€å’Œæœºå™¨è¯­è¨€çš„ä¸€ç§æ˜¾å¼çš„å¯¹é½ã€‚ä¸‹é¢æˆ‘å¯¹åˆ†è¯çš„ä¸€äº›ç»†èŠ‚è¿›ä¸€æ­¥çš„è®¨è®ºã€‚

### 7.2.1 åŸºäºç©ºæ ¼çš„åˆ†è¯

æœ€ç®€å•çš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨`text.split(' ')`æ–¹å¼è¿›è¡Œåˆ†è¯ï¼Œè¿™ç§åˆ†è¯æ–¹å¼å¯¹äºè‹±æ–‡è¿™ç§æŒ‰ç…§ç©ºæ ¼ï¼Œä¸”æ¯ä¸ªåˆ†è¯åçš„å•è¯æœ‰è¯­ä¹‰å…³ç³»çš„æ–‡æœ¬æ˜¯ç®€å•è€Œç›´æ¥çš„åˆ†è¯æ–¹å¼ã€‚ç„¶è€Œï¼Œå¯¹äºä¸€äº›è¯­è¨€ï¼Œå¦‚ä¸­æ–‡ï¼Œå¥å­ä¸­çš„å•è¯ä¹‹é—´æ²¡æœ‰ç©ºæ ¼ï¼š

$$\text{"æˆ‘ä»Šå¤©å»äº†å•†åº—ã€‚"}$$

è¿˜æœ‰ä¸€äº›è¯­è¨€ï¼Œæ¯”å¦‚å¾·è¯­ï¼Œå­˜åœ¨ç€é•¿çš„å¤åˆè¯ï¼ˆä¾‹å¦‚Abwasserbehandlungsanlangeï¼‰ã€‚å³ä½¿åœ¨è‹±è¯­ä¸­ï¼Œä¹Ÿæœ‰è¿å­—ç¬¦è¯ï¼ˆä¾‹å¦‚father-in-lawï¼‰å’Œç¼©ç•¥è¯ï¼ˆä¾‹å¦‚don'tï¼‰ï¼Œå®ƒä»¬éœ€è¦è¢«æ­£ç¡®æ‹†åˆ†ã€‚ä¾‹å¦‚ï¼ŒPenn Treebankå°†don'tæ‹†åˆ†ä¸ºdoå’Œn'tï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨è¯­è¨€ä¸ŠåŸºäºä¿¡æ¯çš„é€‰æ‹©ï¼Œä½†ä¸å¤ªæ˜æ˜¾ã€‚å› æ­¤ï¼Œä»…ä»…é€šè¿‡ç©ºæ ¼æ¥åˆ’åˆ†å•è¯ä¼šå¸¦æ¥å¾ˆå¤šé—®é¢˜ã€‚

é‚£ä¹ˆï¼Œä»€ä¹ˆæ ·çš„åˆ†è¯æ‰æ˜¯å¥½çš„å‘¢ï¼Ÿç›®å‰ä»ç›´è§‰å’Œå·¥ç¨‹å®è·µçš„è§’åº¦æ¥è¯´ï¼š

- é¦–å…ˆæˆ‘ä»¬ä¸å¸Œæœ›æœ‰å¤ªå¤šçš„æ ‡è®°ï¼ˆæç«¯æƒ…å†µï¼šå­—ç¬¦æˆ–å­—èŠ‚ï¼‰ï¼Œå¦åˆ™åºåˆ—ä¼šå˜å¾—éš¾ä»¥å»ºæ¨¡ã€‚
- å…¶æ¬¡æˆ‘ä»¬ä¹Ÿä¸å¸Œæœ›æ ‡è®°è¿‡å°‘ï¼Œå¦åˆ™å•è¯ä¹‹é—´å°±æ— æ³•å…±äº«å‚æ•°ï¼ˆä¾‹å¦‚ï¼Œmother-in-lawå’Œfather-in-lawåº”è¯¥å®Œå…¨ä¸åŒå—ï¼Ÿï¼‰ï¼Œè¿™å¯¹äºå½¢æ€ä¸°å¯Œçš„è¯­è¨€å°¤å…¶æ˜¯ä¸ªé—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œé˜¿æ‹‰ä¼¯è¯­ã€åœŸè€³å…¶è¯­ç­‰ï¼‰ã€‚
- æ¯ä¸ªæ ‡è®°åº”è¯¥æ˜¯ä¸€ä¸ªåœ¨è¯­è¨€æˆ–ç»Ÿè®¡ä¸Šæœ‰æ„ä¹‰çš„å•ä½ã€‚

### 7.2.2 Byte pair encoding 

å°†å­—èŠ‚å¯¹ç¼–ç ï¼ˆ[BPE](https://zh.wikipedia.org/wiki/%E5%AD%97%E8%8A%82%E5%AF%B9%E7%BC%96%E7%A0%81)ï¼‰ç®—æ³•åº”ç”¨äºæ•°æ®å‹ç¼©é¢†åŸŸï¼Œç”¨äºç”Ÿæˆå…¶ä¸­ä¸€ä¸ªæœ€å¸¸ç”¨çš„åˆ†è¯å™¨ã€‚BPEåˆ†è¯å™¨éœ€è¦é€šè¿‡æ¨¡å‹è®­ç»ƒæ•°æ®è¿›è¡Œå­¦ä¹ ï¼Œè·å¾—éœ€è¦åˆ†è¯æ–‡æœ¬çš„ä¸€äº›é¢‘ç‡ç‰¹å¾ã€‚

å­¦ä¹ åˆ†è¯å™¨çš„è¿‡ç¨‹ï¼Œç›´è§‰ä¸Šï¼Œæˆ‘ä»¬å°†æ¯ä¸ªå­—ç¬¦ä½œä¸ºè‡ªå·±çš„æ ‡è®°ï¼Œå¹¶ç»„åˆé‚£äº›ç»å¸¸å…±åŒå‡ºç°çš„æ ‡è®°ã€‚æ•´ä¸ªè¿‡ç¨‹å¯ä»¥è¡¨ç¤ºä¸ºï¼š

- è¾“å…¥ï¼šè®­ç»ƒè¯­æ–™åº“ï¼ˆå­—ç¬¦åºåˆ—ï¼‰ã€‚
- åˆå§‹åŒ–è¯æ±‡è¡¨ $V$ ä¸ºå­—ç¬¦çš„é›†åˆã€‚
- å½“æˆ‘ä»¬ä»ç„¶å¸Œæœ›Vç»§ç»­å¢é•¿æ—¶ï¼š
  æ‰¾åˆ°$V$ä¸­å…±åŒå‡ºç°æ¬¡æ•°æœ€å¤šçš„å…ƒç´ å¯¹ $x,x'$ ã€‚
- ç”¨ä¸€ä¸ªæ–°çš„ç¬¦å· $xx'$ æ›¿æ¢æ‰€æœ‰ $x,x'$ çš„å‡ºç°ã€‚å°†
-  $xx'$ æ·»åŠ åˆ°Vä¸­ã€‚

è¿™é‡Œä¸¾ä¸€ä¸ªä¾‹å­ï¼š

1. [t, h, e, $\sqcup , c, a, r]$, [t, h, e, $\sqcup , c, a, t],[$ t, h, e, $\sqcup , r, a, t]$

2. [th, e, $\sqcup , c, a, r]$, [th, e, $\sqcup , c, a, t],[$ th, e, $\sqcup , r, a, t]$ (th å‡ºç°äº† 3æ¬¡)

3. [the, $\sqcup , c, a, r]$, [the, $\sqcup , c, a, t],[$ the, $\sqcup , r, a, t]$ (the å‡ºç°äº† 3æ¬¡)

4. [the, $\sqcup , ca, r]$, [the, $\sqcup , ca, t],[$ the, $\sqcup , ca, t]$ (ca å‡ºç°äº† 2æ¬¡)

#### 7.2.2.1 Unicodeçš„é—®é¢˜

Unicodeï¼ˆç»Ÿä¸€ç ï¼‰æ˜¯å½“å‰ä¸»æµçš„ä¸€ç§ç¼–ç æ–¹å¼ã€‚å…¶ä¸­è¿™ç§ç¼–ç æ–¹å¼å¯¹BPEåˆ†è¯äº§ç”Ÿäº†ä¸€ä¸ªé—®é¢˜ï¼ˆå°¤å…¶æ˜¯åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­ï¼‰ï¼ŒUnicodeå­—ç¬¦éå¸¸å¤šï¼ˆå…±144,697ä¸ªå­—ç¬¦ï¼‰ã€‚åœ¨è®­ç»ƒæ•°æ®ä¸­æˆ‘ä»¬ä¸å¯èƒ½è§åˆ°æ‰€æœ‰çš„å­—ç¬¦ã€‚
ä¸ºäº†è¿›ä¸€æ­¥å‡å°‘æ•°æ®çš„ç¨€ç–æ€§ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å­—èŠ‚è€Œä¸æ˜¯Unicodeå­—ç¬¦è¿è¡ŒBPEç®—æ³•ï¼ˆ[Wangç­‰äººï¼Œ2019å¹´](https://arxiv.org/pdf/1909.03341.pdf)ï¼‰ã€‚
ä»¥ä¸­æ–‡ä¸ºä¾‹ï¼š

$$
\text { ä»Šå¤©} \Rightarrow \text {[x62, x11, 4e, ca]}
$$

BPEç®—æ³•åœ¨è¿™é‡Œçš„ä½œç”¨æ˜¯ä¸ºäº†è¿›ä¸€æ­¥å‡å°‘æ•°æ®çš„ç¨€ç–æ€§ã€‚é€šè¿‡å¯¹å­—èŠ‚çº§åˆ«è¿›è¡Œåˆ†è¯ï¼Œå¯ä»¥åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­æ›´å¥½åœ°å¤„ç†Unicodeå­—ç¬¦çš„å¤šæ ·æ€§ï¼Œå¹¶å‡å°‘æ•°æ®ä¸­å‡ºç°çš„ä½é¢‘è¯æ±‡ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ä½¿ç”¨å­—èŠ‚ç¼–ç ï¼Œå¯ä»¥å°†ä¸åŒè¯­è¨€ä¸­çš„è¯æ±‡ç»Ÿä¸€è¡¨ç¤ºä¸ºå­—èŠ‚åºåˆ—ï¼Œä»è€Œæ›´å¥½åœ°å¤„ç†å¤šè¯­è¨€æ•°æ®ã€‚

### 7.2.3 Unigram model (SentencePiece)

ä¸ä»…ä»…æ ¹æ®é¢‘ç‡è¿›è¡Œæ‹†åˆ†ä¸åŒï¼Œä¸€ä¸ªæ›´â€œæœ‰åŸåˆ™â€çš„æ–¹æ³•æ˜¯å®šä¹‰ä¸€ä¸ªç›®æ ‡å‡½æ•°æ¥æ•æ‰ä¸€ä¸ªå¥½çš„åˆ†è¯çš„ç‰¹å¾ï¼Œè¿™ç§åŸºäºç›®æ ‡å‡½æ•°çš„åˆ†è¯æ¨¡å‹å¯ä»¥é€‚åº”æ›´å¥½åˆ†è¯åœºæ™¯ï¼ŒUnigram modelå°±æ˜¯åŸºäºè¿™ç§åŠ¨æœºæå‡ºçš„ã€‚æˆ‘ä»¬ç°åœ¨æè¿°ä¸€ä¸‹unigramæ¨¡å‹ï¼ˆ[Kudoï¼Œ2018å¹´](https://arxiv.org/pdf/1804.10959.pdf)ï¼‰ã€‚

è¿™æ˜¯SentencePieceå·¥å…·ï¼ˆ[Kudoï¼†Richardsonï¼Œ2018å¹´](https://aclanthology.org/D18-2012.pdf)ï¼‰æ‰€æ”¯æŒçš„ä¸€ç§åˆ†è¯æ–¹æ³•ï¼Œä¸BPEä¸€èµ·ä½¿ç”¨ã€‚
å®ƒè¢«ç”¨æ¥è®­ç»ƒT5å’ŒGopheræ¨¡å‹ã€‚ç»™å®šä¸€ä¸ªåºåˆ— $x_{1:L}$ ï¼Œä¸€ä¸ªåˆ†è¯å™¨ $T$ æ˜¯ $p\left(x_{1: L}\right)=\prod_{(i, j) \in T} p\left(x_{i: j}\right)$ çš„ä¸€ä¸ªé›†åˆã€‚è¿™è¾¹ç»™å‡ºä¸€ä¸ªå®ä¾‹ï¼š

- è®­ç»ƒæ•°æ®ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼š $ğ–ºğ–»ğ–ºğ–»ğ–¼$
- åˆ†è¯ç»“æœ  $T={(1,2),(3,4),(5,5)}$ ï¼ˆå…¶ä¸­$V=\{ğ–ºğ–»,ğ–¼\}$ï¼‰
- ä¼¼ç„¶å€¼ï¼š $p(x_{1:L})=2/3â‹…2/3â‹…1/3=4/9$

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œè®­ç»ƒæ•°æ®æ˜¯å­—ç¬¦ä¸²" $ğ–ºğ–»ğ–ºğ–»ğ–¼$ "ã€‚åˆ†è¯ç»“æœ  $T={(1,2),(3,4),(5,5)}$  è¡¨ç¤ºå°†å­—ç¬¦ä¸²æ‹†åˆ†æˆä¸‰ä¸ªå­åºåˆ—ï¼š $(ğ–º,ğ–»)ï¼Œ(ğ–º,ğ–»)ï¼Œ(ğ–¼)$ ã€‚è¯æ±‡è¡¨ $V=\{ğ–ºğ–»,ğ–¼\}$ è¡¨ç¤ºäº†è®­ç»ƒæ•°æ®ä¸­å‡ºç°çš„æ‰€æœ‰è¯æ±‡ã€‚

ä¼¼ç„¶å€¼  $p(x_{1:L})$ æ˜¯æ ¹æ® unigram æ¨¡å‹è®¡ç®—å¾—å‡ºçš„æ¦‚ç‡ï¼Œè¡¨ç¤ºè®­ç»ƒæ•°æ®çš„ä¼¼ç„¶åº¦ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ¦‚ç‡çš„è®¡ç®—ä¸º  $2/3â‹…2/3â‹…1/3=4/9$ ã€‚è¿™ä¸ªå€¼ä»£è¡¨äº†æ ¹æ® unigram æ¨¡å‹ï¼Œå°†è®­ç»ƒæ•°æ®åˆ†è¯ä¸ºæ‰€ç»™çš„åˆ†è¯ç»“æœ $T $çš„æ¦‚ç‡ã€‚

unigram æ¨¡å‹é€šè¿‡ç»Ÿè®¡æ¯ä¸ªè¯æ±‡åœ¨è®­ç»ƒæ•°æ®ä¸­çš„å‡ºç°æ¬¡æ•°æ¥ä¼°è®¡å…¶æ¦‚ç‡ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ $ğ–ºğ–»$ åœ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°äº†ä¸¤æ¬¡ï¼Œ $ğ–¼$ å‡ºç°äº†ä¸€æ¬¡ã€‚å› æ­¤ï¼Œæ ¹æ® unigram æ¨¡å‹çš„ä¼°è®¡ï¼Œ $p(ğ–ºğ–»)=2/3$ ï¼Œ $p(ğ–¼)=1/3$ ã€‚é€šè¿‡å°†å„ä¸ªè¯æ±‡çš„æ¦‚ç‡ç›¸ä¹˜ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æ•´ä¸ªè®­ç»ƒæ•°æ®çš„ä¼¼ç„¶å€¼ä¸º $4/9$ ã€‚

ä¼¼ç„¶å€¼çš„è®¡ç®—æ˜¯ unigram æ¨¡å‹ä¸­é‡è¦çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒç”¨äºè¯„ä¼°åˆ†è¯ç»“æœçš„è´¨é‡ã€‚è¾ƒé«˜çš„ä¼¼ç„¶å€¼è¡¨ç¤ºè®­ç»ƒæ•°æ®ä¸åˆ†è¯ç»“æœä¹‹é—´çš„åŒ¹é…ç¨‹åº¦è¾ƒé«˜ï¼Œè¿™æ„å‘³ç€è¯¥åˆ†è¯ç»“æœè¾ƒä¸ºå‡†ç¡®æˆ–åˆç†ã€‚

#### 7.2.3.1 ç®—æ³•æµç¨‹

- ä»ä¸€ä¸ªâ€œç›¸å½“å¤§â€çš„ç§å­è¯æ±‡è¡¨ $V$ å¼€å§‹ã€‚
- é‡å¤ä»¥ä¸‹æ­¥éª¤ï¼š
  - ç»™å®š $V$ ï¼Œä½¿ç”¨EMç®—æ³•ä¼˜åŒ– $p(x)$ å’Œ $T$ ã€‚
  - è®¡ç®—æ¯ä¸ªè¯æ±‡ $xâˆˆV$ çš„ $loss(x)$ ï¼Œè¡¡é‡å¦‚æœå°† $x$ ä» $V$ ä¸­ç§»é™¤ï¼Œä¼¼ç„¶å€¼ä¼šå‡å°‘å¤šå°‘ã€‚
  - æŒ‰ç…§ $loss$ è¿›è¡Œæ’åºï¼Œå¹¶ä¿ç•™ $V$ ä¸­æ’åé å‰çš„80%çš„è¯æ±‡ã€‚

è¿™ä¸ªè¿‡ç¨‹æ—¨åœ¨ä¼˜åŒ–è¯æ±‡è¡¨ï¼Œå‰”é™¤å¯¹ä¼¼ç„¶å€¼è´¡çŒ®è¾ƒå°çš„è¯æ±‡ï¼Œä»¥å‡å°‘æ•°æ®çš„ç¨€ç–æ€§ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ•ˆæœã€‚é€šè¿‡è¿­ä»£ä¼˜åŒ–å’Œå‰ªæï¼Œè¯æ±‡è¡¨ä¼šé€æ¸æ¼”åŒ–ï¼Œä¿ç•™é‚£äº›å¯¹äºä¼¼ç„¶å€¼æœ‰è¾ƒå¤§è´¡çŒ®çš„è¯æ±‡ï¼Œæå‡æ¨¡å‹çš„æ€§èƒ½ã€‚

## 7.3 æ¨¡å‹æ¶æ„

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»å°†è¯­è¨€æ¨¡å‹å®šä¹‰ä¸ºå¯¹æ ‡è®°åºåˆ—çš„æ¦‚ç‡åˆ†å¸ƒ $p(x_{1},â€¦,x_{L})$ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°è¿™ç§å®šä¹‰éå¸¸ä¼˜é›…ä¸”å¼ºå¤§ï¼ˆé€šè¿‡æç¤ºï¼Œè¯­è¨€æ¨¡å‹åŸåˆ™ä¸Šå¯ä»¥å®Œæˆä»»ä½•ä»»åŠ¡ï¼Œæ­£å¦‚GPT-3æ‰€ç¤ºï¼‰ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œå¯¹äºä¸“é—¨çš„ä»»åŠ¡æ¥è¯´ï¼Œé¿å…ç”Ÿæˆæ•´ä¸ªåºåˆ—çš„ç”Ÿæˆæ¨¡å‹å¯èƒ½æ›´é«˜æ•ˆã€‚

ä¸Šä¸‹æ–‡å‘é‡è¡¨å¾ (Contextual Embedding): ä½œä¸ºå…ˆå†³æ¡ä»¶ï¼Œä¸»è¦çš„å…³é”®å‘å±•æ˜¯å°†æ ‡è®°åºåˆ—ä¸ç›¸åº”çš„ä¸Šä¸‹æ–‡çš„å‘é‡è¡¨å¾ï¼š
$$
[the, mouse, ate, the, cheese] \stackrel{\phi}{\Rightarrow}\left[\left(\begin{array}{c}
1 \\
0.1
\end{array}\right),\left(\begin{array}{l}
0 \\
1
\end{array}\right),\left(\begin{array}{l}
1 \\
1
\end{array}\right),\left(\begin{array}{c}
1 \\
-0.1
\end{array}\right),\left(\begin{array}{c}
0 \\
-1
\end{array}\right)\right]. 
$$
æ­£å¦‚åç§°æ‰€ç¤ºï¼Œæ ‡è®°çš„ä¸Šä¸‹æ–‡å‘é‡è¡¨å¾å–å†³äºå…¶ä¸Šä¸‹æ–‡ï¼ˆå‘¨å›´çš„å•è¯ï¼‰ï¼›ä¾‹å¦‚ï¼Œè€ƒè™‘mouseçš„å‘é‡è¡¨ç¤ºéœ€è¦å…³æ³¨åˆ°å‘¨å›´æŸä¸ªçª—å£å¤§å°çš„å…¶ä»–å•è¯ã€‚

- ç¬¦å·è¡¨ç¤ºï¼šæˆ‘ä»¬å°† $Ï•:V^{L}â†’â„^{dÃ—L}$ å®šä¹‰ä¸ºåµŒå…¥å‡½æ•°ï¼ˆç±»ä¼¼äºåºåˆ—çš„ç‰¹å¾æ˜ å°„ï¼Œæ˜ å°„ä¸ºå¯¹åº”çš„å‘é‡è¡¨ç¤ºï¼‰ã€‚

- å¯¹äºæ ‡è®°åºåˆ— $x1:L=[x_{1},â€¦,x_{L}]$ï¼Œ$Ï•$ ç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡è¡¨å¾ $Ï•(x_{1:L})$ã€‚

### 7.3.1 è¯­è¨€æ¨¡å‹åˆ†ç±»

å¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œæœ€åˆçš„èµ·æºæ¥è‡ªäºTransformeræ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹æ˜¯ç¼–ç -è§£ç ç«¯ ï¼ˆEncoder-Decoderï¼‰çš„æ¶æ„ã€‚ä½†æ˜¯å½“å‰å¯¹äºè¯­è¨€æ¨¡å‹çš„åˆ†ç±»ï¼Œå°†è¯­è¨€æ¨¡å‹åˆ†ä¸ºä¸‰ä¸ªç±»å‹ï¼šç¼–ç ç«¯ï¼ˆEncoder-Onlyï¼‰ï¼Œè§£ç ç«¯ï¼ˆDecoder-Onlyï¼‰å’Œç¼–ç -è§£ç ç«¯ï¼ˆEncoder-Decoderï¼‰ã€‚å› æ­¤æˆ‘ä»¬çš„æ¶æ„å±•ç¤ºä»¥å½“å‰çš„åˆ†ç±»å±•å¼€ã€‚

#### 7.3.1.1 ç¼–ç ç«¯ï¼ˆEncoder-Onlyï¼‰æ¶æ„

ç¼–ç ç«¯æ¶æ„çš„è‘—åçš„æ¨¡å‹å¦‚BERTã€RoBERTaç­‰ã€‚è¿™äº›è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡è¡¨å¾ï¼Œä½†ä¸èƒ½ç›´æ¥ç”¨äºç”Ÿæˆæ–‡æœ¬ã€‚å¯ä»¥è¡¨ç¤ºä¸ºï¼Œ$x_{1:L}â‡’Ï•(x_{1:L})$ã€‚è¿™äº›ä¸Šä¸‹æ–‡å‘é‡è¡¨å¾é€šå¸¸ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼ˆä¹Ÿå‘—ç§°ä¸ºè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ï¼‰ã€‚ä»»åŠ¡å½¢å¼æ¯”è¾ƒç®€å•ï¼Œä¸‹é¢ä»¥æƒ…æ„Ÿåˆ†ç±»/è‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ä¸¾ä¾‹ï¼š
$$
æƒ…æ„Ÿåˆ†æè¾“å…¥ä¸è¾“å‡ºå½¢å¼ï¼š[[CLS], ä»–ä»¬, ç§»åŠ¨, è€Œ, å¼ºå¤§]\Rightarrow æ­£é¢æƒ…ç»ª
$$

$$
è‡ªç„¶è¯­è¨€å¤„ç†è¾“å…¥ä¸è¾“å‡ºå½¢å¼ï¼š[[CLS], æ‰€æœ‰, åŠ¨ç‰©, éƒ½, å–œæ¬¢, åƒ, é¥¼å¹², å“¦]â‡’è•´æ¶µ
$$

è¯¥æ¶æ„çš„ä¼˜åŠ¿æ˜¯å¯¹äºæ–‡æœ¬çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æœ‰æ›´å¥½çš„ç†è§£ï¼Œå› æ­¤è¯¥æ¨¡å‹æ¶æ„æ‰ä¼šå¤šç”¨äºç†è§£ä»»åŠ¡ã€‚è¯¥æ¶æ„çš„æœ‰ç‚¹æ˜¯å¯¹äºæ¯ä¸ª $x{i}$ï¼Œä¸Šä¸‹æ–‡å‘é‡è¡¨å¾å¯ä»¥åŒå‘åœ°ä¾èµ–äºå·¦ä¾§ä¸Šä¸‹æ–‡ $(x_{1:iâˆ’1})$ å’Œå³ä¾§ä¸Šä¸‹æ–‡ $(x_{i+1:L})$ã€‚ä½†æ˜¯ç¼ºç‚¹åœ¨äºä¸èƒ½è‡ªç„¶åœ°ç”Ÿæˆå®Œæˆæ–‡æœ¬ï¼Œä¸”éœ€è¦æ›´å¤šçš„ç‰¹å®šè®­ç»ƒç›®æ ‡ï¼ˆå¦‚æ©ç è¯­è¨€å»ºæ¨¡ï¼‰ã€‚

#### 7.3.3.2 è§£ç å™¨ï¼ˆDecoder-Onlyï¼‰æ¶æ„

è§£ç å™¨æ¶æ„çš„è‘—åæ¨¡å‹å°±æ˜¯å¤§åé¼é¼çš„GPTç³»åˆ—æ¨¡å‹ã€‚è¿™äº›æ˜¯æˆ‘ä»¬å¸¸è§çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œç»™å®šä¸€ä¸ªæç¤º $x_{1:i}$ï¼Œå®ƒä»¬å¯ä»¥ç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡è¡¨å¾ï¼Œå¹¶å¯¹ä¸‹ä¸€ä¸ªæ ‡è®° $x_{i+1}$ï¼ˆä»¥åŠé€’å½’åœ°ï¼Œæ•´ä¸ªå®Œæˆ $x_{i+1:L}$ï¼‰ç”Ÿæˆä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒã€‚$x_{1:i}â‡’Ï•(x_{1:i}),p(x_{i+1}âˆ£x_{1:i})$ã€‚æˆ‘ä»¬ä»¥è‡ªåŠ¨è¡¥å…¨ä»»åŠ¡æ¥è¯´ï¼Œè¾“å…¥ä¸è¾“å‡ºçš„å½¢å¼ä¸ºï¼Œ$[[CLS], ä»–ä»¬, ç§»åŠ¨, è€Œ]â‡’å¼ºå¤§$ã€‚ä¸ç¼–ç ç«¯æ¶æ„æ¯”ï¼Œå…¶ä¼˜ç‚¹ä¸ºèƒ½å¤Ÿè‡ªç„¶åœ°ç”Ÿæˆå®Œæˆæ–‡æœ¬ï¼Œæœ‰ç®€å•çš„è®­ç»ƒç›®æ ‡ï¼ˆæœ€å¤§ä¼¼ç„¶ï¼‰ã€‚ç¼ºç‚¹ä¹Ÿå¾ˆæ˜æ˜¾ï¼Œå¯¹äºæ¯ä¸ª $xi$ï¼Œä¸Šä¸‹æ–‡å‘é‡è¡¨å¾åªèƒ½å•å‘åœ°ä¾èµ–äºå·¦ä¾§ä¸Šä¸‹æ–‡ ($x_{1:iâˆ’1}$)ã€‚

####  7.3.3.3 ç¼–ç -è§£ç ç«¯ï¼ˆEncoder-Decoderï¼‰æ¶æ„

ç¼–ç -è§£ç ç«¯æ¶æ„å°±æ˜¯æœ€åˆçš„Transformeræ¨¡å‹ï¼Œå…¶ä»–çš„è¿˜æœ‰å¦‚BARTã€T5ç­‰æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹åœ¨æŸç§ç¨‹åº¦ä¸Šç»“åˆäº†ä¸¤è€…çš„ä¼˜ç‚¹ï¼šå®ƒä»¬å¯ä»¥ä½¿ç”¨åŒå‘ä¸Šä¸‹æ–‡å‘é‡è¡¨å¾æ¥å¤„ç†è¾“å…¥ $x_{1:L}$ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆè¾“å‡º $y_{1:L}$ã€‚å¯ä»¥å…¬å¼åŒ–ä¸ºï¼š
$$
x1:Lâ‡’Ï•(x1:L),p(y1:Lâˆ£Ï•(x1:L))ã€‚
$$
ä»¥è¡¨æ ¼åˆ°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸ºä¾‹ï¼Œå…¶è¾“å…¥å’Œè¾“å‡ºçš„å¯ä»¥è¡¨ç¤ºä¸ºï¼š
$$
[åç§°:, æ¤ç‰©, |, ç±»å‹:, èŠ±å‰, å•†åº—]â‡’[èŠ±å‰, æ˜¯, ä¸€, ä¸ª, å•†åº—]ã€‚
$$

è¯¥æ¨¡å‹çš„å…·æœ‰ç¼–ç ç«¯ï¼Œè§£ç ç«¯ä¸¤ä¸ªæ¶æ„çš„å…±åŒçš„ä¼˜ç‚¹ï¼Œå¯¹äºæ¯ä¸ª $x_{i}$ï¼Œä¸Šä¸‹æ–‡å‘é‡è¡¨å¾å¯ä»¥åŒå‘åœ°ä¾èµ–äºå·¦ä¾§ä¸Šä¸‹æ–‡ ($x_{1:iâˆ’1}$) å’Œå³ä¾§ä¸Šä¸‹æ–‡ ($x_{i+1:L}$)ï¼Œå¯ä»¥è‡ªç”±çš„ç”Ÿæˆæ–‡æœ¬æ•°æ®ã€‚ç¼ºç‚¹å°±è¯´éœ€è¦æ›´å¤šçš„ç‰¹å®šè®­ç»ƒç›®æ ‡ã€‚

### 7.3.4 è¯­è¨€æ¨¡å‹ç†è®º

ä¸‹ä¸€æ­¥ï¼Œæˆ‘ä»¬ä¼šä»‹ç»è¯­è¨€æ¨¡å‹çš„æ¨¡å‹æ¶æ„ï¼Œé‡ç‚¹ä»‹ç»Transformeræ¶æ„æœºå™¨å»¶ä¼¸çš„å†…å®¹ã€‚å¦å¤–æˆ‘ä»¬å¯¹äºæ¶æ„è¿˜ä¼šå¯¹äºä¹‹å‰RNNç½‘ç»œçš„æ ¸å¿ƒçŸ¥è¯†è¿›è¡Œé˜è¿°ï¼Œå…¶ç›®çš„æ˜¯å¯¹äºä»£è¡¨æ€§çš„æ¨¡å‹æ¶æ„è¿›è¡Œå­¦ä¹ ï¼Œä¸ºæœªæ¥çš„å†…å®¹å¢åŠ çŸ¥è¯†å‚¨å¤‡ã€‚

æ·±åº¦å­¦ä¹ çš„ç¾å¦™ä¹‹å¤„åœ¨äºèƒ½å¤Ÿåˆ›å»ºæ„å»ºæ¨¡å—ï¼Œå°±åƒæˆ‘ä»¬ç”¨å‡½æ•°æ„å»ºæ•´ä¸ªç¨‹åºä¸€æ ·ã€‚å› æ­¤ï¼Œåœ¨ä¸‹é¢çš„æ¨¡å‹æ¶æ„çš„è®²è¿°ä¸­ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåƒä¸‹é¢çš„å‡½æ•°ä¸€æ ·å°è£…ï¼Œä»¥å‡½æ•°çš„çš„æ–¹æ³•è¿›è¡Œç†è§£:
$$
TransformerBlock(x_{1:L})
$$
ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†åœ¨å‡½æ•°ä¸»ä½“ä¸­åŒ…å«å‚æ•°ï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªæ„å»ºæ¨¡å—åº“ï¼Œç›´åˆ°æ„å»ºå®Œæ•´çš„Transformeræ¨¡å‹ã€‚

#### 7.3.4.1 åŸºç¡€æ¶æ„

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å°†æ ‡è®°åºåˆ—è½¬æ¢ä¸ºåºåˆ—çš„å‘é‡å½¢å¼ã€‚$EmbedToken$å‡½æ•°é€šè¿‡åœ¨åµŒå…¥çŸ©é˜µ$Eâˆˆâ„^{|v|Ã—d}$ä¸­æŸ¥æ‰¾æ¯ä¸ªæ ‡è®°æ‰€å¯¹åº”çš„å‘é‡ï¼Œè¯¥å‘é‡çš„å…·ä½“å€¼è¿™æ˜¯ä»æ•°æ®ä¸­å­¦ä¹ çš„å‚æ•°ï¼š

def $EmbedToken(x_{1:L}:V^{L})â†’â„^{dÃ—L}$ï¼š

- å°†åºåˆ—$x_{1:L}$ä¸­çš„æ¯ä¸ªæ ‡è®°$xi$è½¬æ¢ä¸ºå‘é‡ã€‚
- è¿”å›[Ex1,â€¦,ExL]ã€‚

ä»¥ä¸Šçš„è¯åµŒå…¥æ˜¯ä¼ ç»Ÿçš„è¯åµŒå…¥ï¼Œå‘é‡å†…å®¹ä¸ä¸Šä¸‹æ–‡æ— å…³ã€‚è¿™é‡Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæŠ½è±¡çš„$SequenceModel$å‡½æ•°ï¼Œå®ƒæ¥å—è¿™äº›ä¸Šä¸‹æ–‡æ— å…³çš„åµŒå…¥ï¼Œå¹¶å°†å®ƒä»¬æ˜ å°„ä¸ºä¸Šä¸‹æ–‡ç›¸å…³çš„åµŒå…¥ã€‚

$def SequenceModel(x_{1:L}:â„^{dÃ—L})â†’â„^{dÃ—L}$ï¼š

- é’ˆå¯¹åºåˆ—$x_{1:L}$ä¸­çš„æ¯ä¸ªå…ƒç´ xiè¿›è¡Œå¤„ç†ï¼Œè€ƒè™‘å…¶ä»–å…ƒç´ ã€‚
- [æŠ½è±¡å®ç°ï¼ˆä¾‹å¦‚ï¼Œ$FeedForwardSequenceModel$ï¼Œ$SequenceRNN$ï¼Œ$TransformerBlock$ï¼‰]

æœ€ç®€å•ç±»å‹çš„åºåˆ—æ¨¡å‹åŸºäºå‰é¦ˆç½‘ç»œï¼ˆBengioç­‰äººï¼Œ2003ï¼‰ï¼Œåº”ç”¨äºå›ºå®šé•¿åº¦çš„ä¸Šä¸‹æ–‡ï¼Œå°±åƒn-gramæ¨¡å‹ä¸€æ ·ï¼Œå‡½æ•°çš„å®ç°å¦‚ä¸‹ï¼š

def $FeedForwardSequenceModel(x_{1:L}:â„^{dÃ—L})â†’â„^{dÃ—L}$ï¼š

- é€šè¿‡æŸ¥çœ‹æœ€å$n$ä¸ªå…ƒç´ å¤„ç†åºåˆ—$x_{1:L}$ä¸­çš„æ¯ä¸ªå…ƒç´ $xi$ã€‚
- å¯¹äºæ¯ä¸ª$i=1,â€¦,L$ï¼š
  - è®¡ç®—$h_{i}$=$FeedForward(x_{iâˆ’n+1},â€¦,x_{i})$ã€‚
- è¿”å›[$h_{1},â€¦,h_{L}$]ã€‚

#### 7.3.4.2 é€’å½’ç¥ç»ç½‘ç»œ
ç¬¬ä¸€ä¸ªçœŸæ­£çš„åºåˆ—æ¨¡å‹æ˜¯é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œå®ƒæ˜¯ä¸€ç±»æ¨¡å‹ï¼ŒåŒ…æ‹¬ç®€å•çš„RNNã€LSTMå’ŒGRUã€‚åŸºæœ¬å½¢å¼çš„RNNé€šè¿‡é€’å½’åœ°è®¡ç®—ä¸€ç³»åˆ—éšè—çŠ¶æ€æ¥è¿›è¡Œè®¡ç®—ã€‚

def $SequenceRNN(x:â„^{dÃ—L})â†’â„^{dÃ—L}$ï¼š

- ä»å·¦åˆ°å³å¤„ç†åºåˆ—$x_{1},â€¦,x_{L}$ï¼Œå¹¶é€’å½’è®¡ç®—å‘é‡$h_{1},â€¦,h_{L}$ã€‚
- å¯¹äº$i=1,â€¦,L$ï¼š
  - è®¡ç®—$h_{i}=RNN(h_{iâˆ’1},x_{i})$ã€‚
  - è¿”å›$[h_{1},â€¦,h_{L}]$ã€‚

å®é™…å®Œæˆå·¥ä½œçš„æ¨¡å—æ˜¯RNNï¼Œç±»ä¼¼äºæœ‰é™çŠ¶æ€æœºï¼Œå®ƒæ¥æ”¶å½“å‰çŠ¶æ€hã€æ–°è§‚æµ‹å€¼xï¼Œå¹¶è¿”å›æ›´æ–°åçš„çŠ¶æ€ï¼š

def $RNN(h:â„^d,x:â„^d)â†’â„^d$ï¼š

- æ ¹æ®æ–°çš„è§‚æµ‹å€¼xæ›´æ–°éšè—çŠ¶æ€hã€‚
- [æŠ½è±¡å®ç°ï¼ˆä¾‹å¦‚ï¼ŒSimpleRNNï¼ŒLSTMï¼ŒGRUï¼‰]

æœ‰ä¸‰ç§æ–¹æ³•å¯ä»¥å®ç°RNNã€‚æœ€æ—©çš„RNNæ˜¯ç®€å•RNNï¼ˆ[Elmanï¼Œ1990](https://onlinelibrary.wiley.com/doi/epdf/10.1207/s15516709cog1402_1)ï¼‰ï¼Œå®ƒå°†$h$å’Œ$x$çš„çº¿æ€§ç»„åˆé€šè¿‡é€å…ƒç´ éçº¿æ€§å‡½æ•°$Ïƒ$ï¼ˆä¾‹å¦‚ï¼Œé€»è¾‘å‡½æ•°$Ïƒ(z)=(1+eâˆ’z)âˆ’1$æˆ–æ›´ç°ä»£çš„$ReLU$å‡½æ•°$Ïƒ(z)=max(0,z)$ï¼‰è¿›è¡Œå¤„ç†ã€‚

def $SimpleRNN(h:â„d,x:â„d)â†’â„d$ï¼š

- é€šè¿‡ç®€å•çš„çº¿æ€§å˜æ¢å’Œéçº¿æ€§å‡½æ•°æ ¹æ®æ–°çš„è§‚æµ‹å€¼$x$æ›´æ–°éšè—çŠ¶æ€$h$ã€‚
- è¿”å›$Ïƒ(Uh+Vx+b)$ã€‚

æ­£å¦‚å®šä¹‰çš„RNNåªä¾èµ–äºè¿‡å»ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡å‘åè¿è¡Œå¦ä¸€ä¸ªRNNæ¥ä½¿å…¶ä¾èµ–äºæœªæ¥ä¸¤ä¸ªã€‚è¿™äº›æ¨¡å‹è¢«ELMoå’ŒULMFiTä½¿ç”¨ã€‚

def $BidirectionalSequenceRNN(x_{1:L}:â„^{dÃ—L})â†’â„^{2dÃ—L}$\ï¼š

- åŒæ—¶ä»å·¦åˆ°å³å’Œä»å³åˆ°å·¦å¤„ç†åºåˆ—ã€‚
- è®¡ç®—ä»å·¦åˆ°å³ï¼š$[hâ†’_{1},â€¦,hâ†’_{L}]â†SequenceRNN(x_{1},â€¦,x_{L})$ã€‚
- è®¡ç®—ä»å³åˆ°å·¦ï¼š$[hâ†_{L},â€¦,hâ†_{1}]â†SequenceRNN(x_{L},â€¦,x_{1})$ã€‚
- è¿”å›$[hâ†’_{1}hâ†_{1},â€¦,hâ†’_{L}hâ†_{L}]$ã€‚

æ³¨ï¼š

- ç®€å•RNNç”±äºæ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜å¾ˆéš¾è®­ç»ƒã€‚
- ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå‘å±•äº†é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰å’Œé—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰ï¼ˆéƒ½å±äºRNNï¼‰ã€‚
- ç„¶è€Œï¼Œå³ä½¿åµŒå…¥h200å¯ä»¥ä¾èµ–äºä»»æ„è¿œçš„è¿‡å»ï¼ˆä¾‹å¦‚ï¼Œx1ï¼‰ï¼Œå®ƒä¸å¤ªå¯èƒ½ä»¥â€œç²¾ç¡®â€çš„æ–¹å¼ä¾èµ–äºå®ƒï¼ˆæ›´å¤šè®¨è®ºï¼Œè¯·å‚è§Khandelwalç­‰äººï¼Œ2018ï¼‰ã€‚
- ä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼ŒLSTMçœŸæ­£åœ°å°†æ·±åº¦å­¦ä¹ å¼•å…¥äº†NLPé¢†åŸŸã€‚

#### 7.3.4.3 Transformer
ç°åœ¨ï¼Œæˆ‘ä»¬å°†è®¨è®ºTransformerï¼ˆ[Vaswaniç­‰äººï¼Œ2017](https://arxiv.org/pdf/1706.03762.pdf)ï¼‰ï¼Œè¿™æ˜¯çœŸæ­£æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹å‘å±•çš„åºåˆ—æ¨¡å‹ã€‚æ­£å¦‚ä¹‹å‰æ‰€æåˆ°çš„ï¼ŒTransformeræ¨¡å‹å°†å…¶åˆ†è§£ä¸ºEncoder-Onlyï¼ˆGPT-2ï¼ŒGPT-3ï¼‰ã€Decoder-Onlyï¼ˆBERTï¼ŒRoBERTaï¼‰å’ŒEncoder-Decoderï¼ˆBARTï¼ŒT5ï¼‰æ¨¡å‹çš„æ„å»ºæ¨¡å—ã€‚

å…³äºTransformerçš„å­¦ä¹ èµ„æºæœ‰å¾ˆå¤šï¼š

- [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)å’Œ[Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)ï¼šå¯¹Transformerçš„è§†è§‰æè¿°éå¸¸å¥½ã€‚
- [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)ï¼šTransformerçš„Pytorchå®ç°ã€‚

å¼ºçƒˆå»ºè®®æ‚¨é˜…è¯»è¿™äº›å‚è€ƒèµ„æ–™ã€‚è¯¥è¯¾ç¨‹ä¸»è¦ä¾æ®ä»£ç å‡½æ•°å’Œæ¥å£è¿›è¡Œè®²è§£ã€‚

##### 7.3.4.3.1 æ³¨æ„åŠ›æœºåˆ¶

Transformerçš„å…³é”®æ˜¯æ³¨æ„æœºåˆ¶ï¼Œè¿™ä¸ªæœºåˆ¶æ—©åœ¨æœºå™¨ç¿»è¯‘ä¸­å°±è¢«å¼€å‘å‡ºæ¥äº†ï¼ˆBahdananuç­‰äººï¼Œ2017ï¼‰ã€‚å¯ä»¥å°†æ³¨æ„åŠ›è§†ä¸ºä¸€ä¸ªâ€œè½¯â€æŸ¥æ‰¾è¡¨ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªæŸ¥è¯¢$y$ï¼Œæˆ‘ä»¬å¸Œæœ›å°†å…¶ä¸åºåˆ—$x_{1:L}=[x_1,â€¦,x_L]$çš„æ¯ä¸ªå…ƒç´ è¿›è¡ŒåŒ¹é…ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡çº¿æ€§å˜æ¢å°†æ¯ä¸ª$x_{i}$è§†ä¸ºè¡¨ç¤ºé”®å€¼å¯¹ï¼š
$$
(W_{key}x_{i})ï¼š(W_{value}x_{i})
$$

å¹¶é€šè¿‡å¦ä¸€ä¸ªçº¿æ€§å˜æ¢å½¢æˆæŸ¥è¯¢ï¼š
$$
W_{query}y
$$
å¯ä»¥å°†é”®å’ŒæŸ¥è¯¢è¿›è¡Œæ¯”è¾ƒï¼Œå¾—åˆ°ä¸€ä¸ªåˆ†æ•°ï¼š
$$
score_{i}=x^{âŠ¤}_{i}W^{âŠ¤}_{key}W_{query}y
$$
è¿™äº›åˆ†æ•°å¯ä»¥è¿›è¡ŒæŒ‡æ•°åŒ–å’Œå½’ä¸€åŒ–ï¼Œå½¢æˆå…³äºæ ‡è®°ä½ç½®${1,â€¦,L}$çš„æ¦‚ç‡åˆ†å¸ƒï¼š
$$
[Î±_{1},â€¦,Î±_{L}]=softmax([score_{1},â€¦,score_{L}])
$$
ç„¶åæœ€ç»ˆçš„è¾“å‡ºæ˜¯åŸºäºå€¼çš„åŠ æƒç»„åˆï¼š
$$
\sum_{i=1}^L \alpha_i\left(W_{value} x_i\right)
$$
æˆ‘ä»¬å¯ä»¥ç”¨çŸ©é˜µå½¢å¼ç®€æ´åœ°è¡¨ç¤ºæ‰€æœ‰è¿™äº›å†…å®¹ï¼š

def $Attention(x_{1:L}:â„^{dÃ—L},y:â„^d)â†’â„^d$ï¼š

- é€šè¿‡å°†å…¶ä¸æ¯ä¸ª$x_{i}$è¿›è¡Œæ¯”è¾ƒæ¥å¤„ç†$y$ã€‚
- è¿”å›$W_{value} x_{1: L} \operatorname{softmax}\left(x_{1: L}^{\top} W_{key}^{\top} W_{query} y / \sqrt{d}\right)$

æˆ‘ä»¬å¯ä»¥å°†æ³¨æ„åŠ›çœ‹ä½œæ˜¯å…·æœ‰å¤šä¸ªæ–¹é¢ï¼ˆä¾‹å¦‚ï¼Œå¥æ³•ã€è¯­ä¹‰ï¼‰çš„åŒ¹é…ã€‚ä¸ºäº†é€‚åº”è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥åŒæ—¶ä½¿ç”¨å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œå¹¶ç®€å•åœ°ç»„åˆå®ƒä»¬çš„è¾“å‡ºã€‚

def $MultiHeadedAttention(x_{1:L}:â„^{dÃ—L},y:â„^{d})â†’â„^{d}$:

- é€šè¿‡å°†å…¶ä¸æ¯ä¸ªxiä¸nheadsä¸ªæ–¹é¢è¿›è¡Œæ¯”è¾ƒï¼Œå¤„ç†yã€‚
- è¿”å›$W_{output}[\underbrace{\left[\operatorname{Attention}\left(x_{1: L}, y\right), \ldots, \operatorname{Attention}\left(x_{1: L}, y\right)\right]}_{n_{heads}times}$

å¯¹äº**è‡ªæ³¨æ„å±‚**ï¼Œæˆ‘ä»¬å°†ç”¨$x_{i}$æ›¿æ¢$y$ä½œä¸ºæŸ¥è¯¢å‚æ•°æ¥äº§ç”Ÿï¼Œå…¶æœ¬è´¨ä¸Šå°±æ˜¯å°†è‡ªèº«çš„$x_{i}$å¯¹å¥å­çš„å…¶ä»–ä¸Šä¸‹æ–‡å†…å®¹è¿›è¡Œ$Attention$çš„è¿ç®—ï¼š

def $SelfAttention(x_{1:L}:â„_{dÃ—L})â†’â„_{dÃ—L})$ï¼š

- å°†æ¯ä¸ªå…ƒç´ xiä¸å…¶ä»–å…ƒç´ è¿›è¡Œæ¯”è¾ƒã€‚ 
- è¿”å›$[Attention(x_{1:L},x_{1}),â€¦,Attention(x_{1:L},x_{L})]$ã€‚

è‡ªæ³¨æ„åŠ›ä½¿å¾—æ‰€æœ‰çš„æ ‡è®°éƒ½å¯ä»¥â€œç›¸äº’é€šä¿¡â€ï¼Œè€Œ**å‰é¦ˆå±‚**æä¾›è¿›ä¸€æ­¥çš„è¿æ¥ï¼š

def $FeedForward(x_{1:L}:â„^{dÃ—L})â†’â„^{dÃ—L}$ï¼š

- ç‹¬ç«‹å¤„ç†æ¯ä¸ªæ ‡è®°ã€‚
- å¯¹äº$i=1,â€¦,L$ï¼š
  - è®¡ç®—$y_{i}=W_{2}max(W_{1}x_{i}+b_{1},0)+b_{2}$ã€‚
- è¿”å›$[y_{1},â€¦,y_{L}]$ã€‚

å¯¹äºTransformerçš„ä¸»è¦çš„ç»„ä»¶ï¼Œæˆ‘ä»¬å·®ä¸å¤šè¿›è¡Œä»‹ç»ã€‚åŸåˆ™ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥åªéœ€å°†$FeedForwardâˆ˜SelfAttention$åºåˆ—æ¨¡å‹è¿­ä»£96æ¬¡ä»¥æ„å»ºGPT-3ï¼Œä½†æ˜¯é‚£æ ·çš„ç½‘ç»œå¾ˆéš¾ä¼˜åŒ–ï¼ˆåŒæ ·å—åˆ°æ²¿æ·±åº¦æ–¹å‘çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„å›°æ‰°ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»è¿›è¡Œä¸¤ä¸ªæ‰‹æ®µï¼Œä»¥ç¡®ä¿ç½‘ç»œå¯è®­ç»ƒã€‚

##### 7.3.4.3.2 æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–

**æ®‹å·®è¿æ¥**ï¼šè®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªæŠ€å·§æ˜¯æ®‹å·®è¿æ¥ï¼ˆResNetï¼‰ã€‚æˆ‘ä»¬ä¸ä»…åº”ç”¨æŸä¸ªå‡½æ•°fï¼š
$$
f(x1:L)ï¼Œ
$$
è€Œæ˜¯æ·»åŠ ä¸€ä¸ªæ®‹å·®ï¼ˆè·³è·ƒï¼‰è¿æ¥ï¼Œä»¥ä¾¿å¦‚æœ$f$çš„æ¢¯åº¦æ¶ˆå¤±ï¼Œæ¢¯åº¦ä»ç„¶å¯ä»¥é€šè¿‡$x_{1:L}è¿›è¡Œè®¡ç®—ï¼š
$$
x_{1:L}+f(x_{1:L})ã€‚
$$
**å±‚å½’ä¸€åŒ–**:å¦ä¸€ä¸ªæŠ€å·§æ˜¯å±‚å½’ä¸€åŒ–ï¼Œå®ƒæ¥æ”¶ä¸€ä¸ªå‘é‡å¹¶ç¡®ä¿å…¶å…ƒç´ ä¸ä¼šå¤ªå¤§ï¼š

def $LayerNorm(x_{1:L}:â„^{dÃ—L})â†’â„^{dÃ—L}$ï¼š

- ä½¿å¾—æ¯ä¸ª$x_{i}$æ—¢ä¸å¤ªå¤§ä¹Ÿä¸å¤ªå°ã€‚

æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªé€‚é…å™¨å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥å—ä¸€ä¸ªåºåˆ—æ¨¡å‹$f$å¹¶ä½¿å…¶â€œé²æ£’â€ï¼š

def $AddNorm(f:(â„d^{Ã—L}â†’â„^{dÃ—L}),x_{1:L}:â„_{dÃ—L})â†’â„^{dÃ—L}$ï¼š

- å®‰å…¨åœ°å°†fåº”ç”¨äº$x_{1:L}$ã€‚
- è¿”å›$LayerNorm(x_{1:L}+f(x_{1:L}))$ã€‚

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ç®€æ´åœ°å®šä¹‰Transformerå—å¦‚ä¸‹ï¼š

def $TransformerBlock(x_{1:L}:â„^{dÃ—L})â†’â„^{dÃ—L}$ï¼š

- å¤„ç†ä¸Šä¸‹æ–‡ä¸­çš„æ¯ä¸ªå…ƒç´ $x_{i}$ã€‚
- è¿”å›$AddNorm(FeedForward,AddNorm(SelfAttention,x_{1:L}))$ã€‚

##### 7.3.4.3.3 ä½ç½®åµŒå…¥

æœ€åæˆ‘ä»¬å¯¹ç›®å‰è¯­è¨€æ¨¡å‹çš„**ä½ç½®åµŒå…¥**è¿›è¡Œè®¨è®ºã€‚æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œæ ¹æ®å®šä¹‰ï¼Œæ ‡è®°çš„åµŒå…¥ä¸ä¾èµ–äºå…¶åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œå› æ­¤ä¸¤ä¸ªå¥å­ä¸­çš„ğ—†ğ—ˆğ—ğ—Œğ–¾å°†å…·æœ‰ç›¸åŒçš„åµŒå…¥ï¼Œä»è€Œåœ¨å¥å­ä½ç½®çš„è§’åº¦å¿½ç•¥äº†ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œè¿™æ˜¯ä¸åˆç†çš„ã€‚
$$
[ğ—ğ—ğ–¾,ğ—†ğ—ˆğ—ğ—Œğ–¾,ğ–ºğ—ğ–¾,ğ—ğ—ğ–¾,ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾]
[ğ—ğ—ğ–¾,ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾,ğ–ºğ—ğ–¾,ğ—ğ—ğ–¾,ğ—†ğ—ˆğ—ğ—Œğ–¾]
$$
ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä½ç½®ä¿¡æ¯æ·»åŠ åˆ°åµŒå…¥ä¸­ï¼š

def $EmbedTokenWithPosition(x_{1:L}:â„^{dÃ—L})$ï¼š

- æ·»åŠ ä½ç½®ä¿¡æ¯ã€‚
- å®šä¹‰ä½ç½®åµŒå…¥ï¼š
  - å¶æ•°ç»´åº¦ï¼š$P_{i,2j}=sin(i/10000^{2j/dmodel})$
  - å¥‡æ•°ç»´åº¦ï¼š$P_{i,2j+1}=cos(i/10000^{2j/dmodel})$
- è¿”å›$[x_1+P_1,â€¦,x_L+P_L]$ã€‚

ä¸Šé¢çš„å‡½æ•°ä¸­ï¼Œ$i$è¡¨ç¤ºå¥å­ä¸­æ ‡è®°çš„ä½ç½®ï¼Œ$j$è¡¨ç¤ºè¯¥æ ‡è®°çš„å‘é‡è¡¨ç¤ºç»´åº¦ä½ç½®ã€‚

æœ€åæˆ‘ä»¬æ¥èŠä¸€ä¸‹GPT-3ã€‚åœ¨æ‰€æœ‰ç»„ä»¶å°±ä½åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ç®€è¦åœ°å®šä¹‰GPT-3æ¶æ„ï¼Œåªéœ€å°†Transformerå—å †å 96æ¬¡å³å¯ï¼š
$$
GPT-3(x_{1:L})=TransformerBlock^{96}(EmbedTokenWithPosition(x_{1:L}))
$$
æ¶æ„çš„å½¢çŠ¶ï¼ˆå¦‚ä½•åˆ†é…1750äº¿ä¸ªå‚æ•°ï¼‰ï¼š

- éšè—çŠ¶æ€çš„ç»´åº¦ï¼šdmodel=12288
- ä¸­é—´å‰é¦ˆå±‚çš„ç»´åº¦ï¼šdff=4dmodel
- æ³¨æ„å¤´çš„æ•°é‡ï¼šnheads=96
- ä¸Šä¸‹æ–‡é•¿åº¦ï¼šL=2048

è¿™äº›å†³ç­–æœªå¿…æ˜¯æœ€ä¼˜çš„ã€‚[Levineç­‰äººï¼ˆ2020ï¼‰](https://arxiv.org/pdf/2006.12467.pdf)æä¾›äº†ä¸€äº›ç†è®ºä¸Šçš„è¯æ˜ï¼Œè¡¨æ˜GPT-3çš„æ·±åº¦å¤ªæ·±ï¼Œè¿™ä¿ƒä½¿äº†æ›´æ·±ä½†æ›´å®½çš„Jurassicæ¶æ„çš„è®­ç»ƒã€‚

ä¸åŒç‰ˆæœ¬çš„Transformerä¹‹é—´å­˜åœ¨é‡è¦ä½†è¯¦ç»†çš„å·®å¼‚ï¼š

- å±‚å½’ä¸€åŒ–â€œåå½’ä¸€åŒ–â€ï¼ˆåŸå§‹Transformerè®ºæ–‡ï¼‰ä¸â€œå…ˆå½’ä¸€åŒ–â€ï¼ˆGPT-2ï¼‰ï¼Œè¿™å½±å“äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼ˆ[Davisç­‰äººï¼Œ2021](http://proceedings.mlr.press/v139/davis21a/davis21a.pdf)ï¼‰ã€‚
- åº”ç”¨äº†ä¸¢å¼ƒï¼ˆDropoutï¼‰ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
- GPT-3ä½¿ç”¨äº†[sparse Transformer](https://arxiv.org/pdf/1904.10509.pdf)ï¼ˆç¨€é‡Š Transformerï¼‰æ¥å‡å°‘å‚æ•°æ•°é‡ï¼Œå¹¶ä¸ç¨ å¯†å±‚äº¤é”™ä½¿ç”¨ã€‚
- æ ¹æ®Transformerçš„ç±»å‹ï¼ˆEncdoer-Only, Decoder-Only, Encdoer-Decoderï¼‰ï¼Œä½¿ç”¨ä¸åŒçš„æ©ç æ“ä½œã€‚

## å»¶ä¼¸é˜…è¯»

åˆ†è¯:

- [Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP](https://arxiv.org/pdf/2112.10508.pdf). *Sabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias GallÃ©, Arun Raja, Chenglei Si, Wilson Y. Lee, BenoÃ®t Sagot, Samson Tan*. 2021. Comprehensive survey of tokenization.
- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf). *Rico Sennrich, B. Haddow, Alexandra Birch*. ACL 2015. Introduces **byte pair encoding** into NLP. Used by GPT-2, GPT-3.
- [Googleâ€™s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf). *Yonghui Wu, M. Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, M. Krikun, Yuan Cao, Qin Gao, Klaus Macherey, J. Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Y. Kato, Taku Kudo, H. Kazawa, K. Stevens, George Kurian, Nishant Patil, W. Wang, C. Young, Jason R. Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, G. Corrado, Macduff Hughes, J. Dean*. 2016. Introduces **WordPiece**. Used by BERT.
- [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/pdf/1808.06226.pdf). *Taku Kudo, John Richardson*. EMNLP 2018. Introduces **SentencePiece**.

æ¨¡å‹æ¶æ„:

- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Introduces GPT-2.
- [Attention is All you Need](https://arxiv.org/pdf/1706.03762.pdf). *Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin*. NIPS 2017.
- [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [CS224N slides on RNNs](http://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture06-fancy-rnn.pdf)
- [CS224N slides on Transformers](http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture09-transformers.pdf)
- [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/pdf/2108.12409.pdf). *Ofir Press, Noah A. Smith, M. Lewis*. 2021. Introduces **Alibi embeddings**.
- [Transformer-XL: Attentive Language Models beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860.pdf). *Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov*. ACL 2019. Introduces recurrence on Transformers, relative position encoding scheme.
- [Generating Long Sequences with Sparse Transformers](https://arxiv.org/pdf/1904.10509.pdf). *R. Child, Scott Gray, Alec Radford, Ilya Sutskever*. 2019. Introduces **Sparse Transformers**.
- [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/pdf/2006.04768.pdf). *Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma*. 2020. Introduces **Linformers**.
- [Rethinking Attention with Performers](https://arxiv.org/pdf/2009.14794.pdf). *K. Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, TamÃ¡s SarlÃ³s, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, Adrian Weller*. ICLR 2020. Introduces **Performers**.
- [Efficient Transformers: A Survey](https://arxiv.org/pdf/2009.06732.pdf). *Yi Tay, M. Dehghani, Dara Bahri, Donald Metzler*. 2020.

Decoder-only æ¶æ„:

- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). *Alec Radford, Jeff Wu, R. Child, D. Luan, Dario Amodei, Ilya Sutskever*. 2019. Introduces **GPT-2** from OpenAI.
- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf). *Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child, A. Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei*. NeurIPS 2020. Introduces **GPT-3** from OpenAI.
- [Scaling Language Models: Methods, Analysis&Insights from Training Gopher](https://arxiv.org/pdf/2112.11446.pdf). *Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, J. Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, G. V. D. Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, I. Higgins, Antonia Creswell, Nathan McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, D. Budden, Esme Sutherland, K. Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lorraine Li, A. Kuncoro, Aida Nematzadeh, E. Gribovskaya, Domenic Donato, Angeliki Lazaridou, A. Mensch, J. Lespiau, Maria Tsimpoukelli, N. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson dâ€™Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, I. Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, Geoffrey Irving*. 2021. Introduces **Gopher** from DeepMind.
- [Jurassic-1: Technical details and evaluation](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf). *Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham*. 2021. Introduces **Jurassic** from AI21 Labs.

Encoder-only æ¶æ„:

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf). *Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova*. NAACL 2019. Introduces **BERT** from Google.
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf). *Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, Veselin Stoyanov*. 2019. Introduces **RoBERTa** from Facebook.

Encoder-decoder æ¶æ„:

- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf). *M. Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer*. ACL 2019. Introduces **BART** from Facebook.
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf). *Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W. Li, Peter J. Liu*. J. Mach. Learn. Res. 2019. Introduces **T5** from Google.
